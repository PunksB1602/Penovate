{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid CNN-BiLSTM Framework for Real-Time Handwriting Recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Optional, Sequence, Union, Tuple, Dict, Any\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42, deterministic: bool = True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        # torch.use_deterministic_algorithms(True)  # Stricter determinism, may raise if ops unsupported\n",
    "    else:\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n"
     ]
    }
   ],
   "source": [
    "def load_raw_splits(data_dir: Path):\n",
    "    X_train = np.load(data_dir / \"X_train.npy\")\n",
    "    X_val   = np.load(data_dir / \"X_val.npy\")\n",
    "    X_test  = np.load(data_dir / \"X_test.npy\")\n",
    "\n",
    "    y_train = np.load(data_dir / \"y_train.npy\", allow_pickle=True)\n",
    "    y_val   = np.load(data_dir / \"y_val.npy\", allow_pickle=True)\n",
    "    y_test  = np.load(data_dir / \"y_test.npy\", allow_pickle=True)\n",
    "\n",
    "    len_train = np.load(data_dir / \"len_train.npy\")\n",
    "    len_val   = np.load(data_dir / \"len_val.npy\")\n",
    "    len_test  = np.load(data_dir / \"len_test.npy\")\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, len_train, len_val, len_test\n",
    "\n",
    "print(np.unique(np.load(\"data/processed_imu/y_train.npy\", allow_pickle=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(\n",
    "    X_train, y_train, len_train,\n",
    "    X_val,   y_val,   len_val,\n",
    "    X_test,  y_test,  len_test,\n",
    "    batch_size: int,\n",
    "    num_workers: int = 0,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "\n",
    "    train_ds = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.long),\n",
    "        torch.tensor(len_train, dtype=torch.long),\n",
    "    )\n",
    "    val_ds = TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.long),\n",
    "        torch.tensor(len_val, dtype=torch.long),\n",
    "    )\n",
    "    test_ds = TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.long),\n",
    "        torch.tensor(len_test, dtype=torch.long),\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, drop_last=False,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        persistent_workers=(num_workers > 0),\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        persistent_workers=(num_workers > 0),\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        persistent_workers=(num_workers > 0),\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        num_classes: int,\n",
    "        hidden_size: int = 128,\n",
    "        dropout: float = 0.3,\n",
    "        use_batchnorm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv1d(num_features, 64, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(64) if use_batchnorm else nn.Identity()\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128) if use_batchnorm else nn.Identity()\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 128)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, F), lengths: (B,)\n",
    "        if x.ndim != 3:\n",
    "            raise ValueError(f\"Expected x.ndim=3 (B,T,F), got {x.shape}\")\n",
    "        if x.shape[-1] != self.num_features:\n",
    "            raise ValueError(f\"Expected F={self.num_features}, got {x.shape[-1]}\")\n",
    "        if lengths.ndim != 1:\n",
    "            raise ValueError(f\"Expected lengths.ndim=1, got {lengths.shape}\")\n",
    "\n",
    "        lengths = lengths.to(x.device)\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # (B, F, T)\n",
    "        lengths = torch.clamp(lengths, min=1, max=x.shape[2])\n",
    "\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "        # Update lengths for MaxPool1d(kernel=2)\n",
    "        lengths2 = lengths // 2\n",
    "        lengths2 = torch.clamp(lengths2, min=1, max=x.shape[2])\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # (B, T', 128)\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        # Take last valid timestep per sample\n",
    "        idx = (lengths2 - 1).view(-1, 1, 1).expand(-1, 1, x.size(2))\n",
    "        x = x.gather(dim=1, index=idx).squeeze(1)\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    data_dir: Path\n",
    "    out_dir: Path\n",
    "\n",
    "    num_features: int = 18\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 30\n",
    "\n",
    "    hidden_size: int = 128\n",
    "    dropout: float = 0.3\n",
    "    use_batchnorm: bool = True\n",
    "\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "\n",
    "    step_size: int = 8\n",
    "    gamma: float = 0.7\n",
    "    patience: int = 7\n",
    "\n",
    "    seed: int = 42\n",
    "    deterministic: bool = True\n",
    "\n",
    "    mode: str = \"train\"  # \"train\" or \"eval\"\n",
    "\n",
    "    topk: int = 3  # set 0 to disable top-k accuracy\n",
    "\n",
    "def make_run_name(cfg: TrainConfig) -> str:\n",
    "    bn = \"bn\" if cfg.use_batchnorm else \"no_bn\"\n",
    "    drop = f\"drop{int(cfg.dropout * 10):02d}\"   # 0.3 -> drop03, 0.5 -> drop05\n",
    "    return f\"exp_{bn}_bs{cfg.batch_size}_seed{cfg.seed}_{drop}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training and Evaluation Primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_accuracy(logits: torch.Tensor, y: torch.Tensor, k: int) -> float:\n",
    "    if k <= 0:\n",
    "        return float(\"nan\")\n",
    "    k = min(k, logits.shape[1])\n",
    "    topk = torch.topk(logits, k=k, dim=1).indices  # (B, k)\n",
    "    correct = (topk == y.view(-1, 1)).any(dim=1).float().mean().item()\n",
    "    return correct\n",
    "\n",
    "def train_one_epoch(model, loader, device, criterion, optimizer, *, topk: int = 0):\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "    correct, total = 0, 0\n",
    "    topk_sum = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for x, y, lengths in loader:\n",
    "        x, y, lengths = x.to(device), y.to(device), lengths.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x, lengths)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "        if topk > 0:\n",
    "            topk_sum += topk_accuracy(logits, y, topk)\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = loss_sum / max(1, len(loader))\n",
    "    acc = correct / max(1, total)\n",
    "    acck = (topk_sum / max(1, n_batches)) if topk > 0 else float(\"nan\")\n",
    "    return avg_loss, acc, acck\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, criterion, *, topk: int = 0):\n",
    "    model.eval()\n",
    "    loss_sum = 0.0\n",
    "    correct, total = 0, 0\n",
    "    topk_sum = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for x, y, lengths in loader:\n",
    "        x, y, lengths = x.to(device), y.to(device), lengths.to(device)\n",
    "\n",
    "        logits = model(x, lengths)\n",
    "        loss = criterion(logits, y)\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "        y_true.append(y.cpu().numpy())\n",
    "        y_pred.append(pred.cpu().numpy())\n",
    "\n",
    "        if topk > 0:\n",
    "            topk_sum += topk_accuracy(logits, y, topk)\n",
    "        n_batches += 1\n",
    "\n",
    "    y_true = np.concatenate(y_true) if y_true else np.array([], dtype=int)\n",
    "    y_pred = np.concatenate(y_pred) if y_pred else np.array([], dtype=int)\n",
    "\n",
    "    avg_loss = loss_sum / max(1, len(loader))\n",
    "    acc = correct / max(1, total)\n",
    "    acck = (topk_sum / max(1, n_batches)) if topk > 0 else float(\"nan\")\n",
    "\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\") if y_true.size else float(\"nan\")\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average=\"weighted\") if y_true.size else float(\"nan\")\n",
    "\n",
    "    return avg_loss, acc, acck, macro_f1, weighted_f1, y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Plotting Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves_csv(\n",
    "    log_csv: Path,\n",
    "    out_dir: Path,\n",
    "    prefix: str,\n",
    "    *,\n",
    "    ema: Optional[float] = None,\n",
    "    figsize=(3.5, 2.4),\n",
    "    dpi: int = 600,\n",
    "    use_color: bool = False,\n",
    "    show_grid: bool = False,\n",
    "    acc_in_percent: bool = True,\n",
    "    smart_acc_ylim: bool = True,\n",
    "    mark_best_val: bool = False,\n",
    "    ):\n",
    "    rc = {\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times New Roman\", \"STIXGeneral\", \"DejaVu Serif\"],\n",
    "        \"mathtext.fontset\": \"stix\",\n",
    "        \"font.size\": 10,\n",
    "        \"axes.labelsize\": 10,\n",
    "        \"legend.fontsize\": 9,\n",
    "        \"xtick.labelsize\": 9,\n",
    "        \"ytick.labelsize\": 9,\n",
    "        \"lines.linewidth\": 1.6,\n",
    "        \"axes.linewidth\": 1.0,\n",
    "        \"xtick.direction\": \"in\",\n",
    "        \"ytick.direction\": \"in\",\n",
    "        \"xtick.minor.visible\": True,\n",
    "        \"ytick.minor.visible\": True,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"axes.grid\": show_grid,\n",
    "        \"grid.alpha\": 0.15,\n",
    "        \"grid.linestyle\": \"--\",\n",
    "        \"grid.linewidth\": 0.6,\n",
    "        \"pdf.fonttype\": 42,\n",
    "        \"ps.fonttype\": 42,\n",
    "    }\n",
    "\n",
    "    with mpl.rc_context(rc):\n",
    "        log_csv = Path(log_csv)\n",
    "        out_dir = Path(out_dir)\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        df = pd.read_csv(log_csv)\n",
    "        req = [\"epoch\", \"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\"]\n",
    "        missing = [c for c in req if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing columns in {log_csv}: {missing}\")\n",
    "\n",
    "        x = df[\"epoch\"].to_numpy(dtype=float)\n",
    "\n",
    "        def smooth(y: np.ndarray) -> np.ndarray:\n",
    "            y = np.asarray(y, dtype=float)\n",
    "            if ema is None:\n",
    "                return y\n",
    "            a = float(ema)\n",
    "            if not (0.0 < a < 1.0):\n",
    "                raise ValueError(\"ema must be in (0,1) or None\")\n",
    "            ys = np.empty_like(y, dtype=float)\n",
    "            ys[0] = y[0]\n",
    "            for i in range(1, len(y)):\n",
    "                ys[i] = a * y[i] + (1.0 - a) * ys[i - 1]\n",
    "            return ys\n",
    "\n",
    "        tr_loss = smooth(df[\"train_loss\"].to_numpy())\n",
    "        va_loss = smooth(df[\"val_loss\"].to_numpy())\n",
    "        tr_acc  = smooth(df[\"train_acc\"].to_numpy())\n",
    "        va_acc  = smooth(df[\"val_acc\"].to_numpy())\n",
    "\n",
    "        if acc_in_percent:\n",
    "            tr_acc *= 100.0\n",
    "            va_acc *= 100.0\n",
    "\n",
    "        if use_color:\n",
    "            c_train, c_val = \"#004488\", \"#DDAA33\"\n",
    "        else:\n",
    "            c_train, c_val = \"black\", \"black\"\n",
    "\n",
    "        lw_train, lw_val = 1.8, 1.6\n",
    "\n",
    "        def finalize_axes(ax: plt.Axes):\n",
    "            ax.tick_params(which=\"both\", top=False, right=False)\n",
    "            ax.set_xlim(float(np.min(x)), float(np.max(x)))\n",
    "            if show_grid:\n",
    "                ax.grid(True, which=\"major\")\n",
    "                ax.grid(True, which=\"minor\", alpha=0.08)\n",
    "\n",
    "        def save(fig: plt.Figure, base: Path):\n",
    "            fig.savefig(base.with_suffix(\".pdf\"), bbox_inches=\"tight\", pad_inches=0.02)\n",
    "            fig.savefig(base.with_suffix(\".png\"), dpi=dpi, bbox_inches=\"tight\", pad_inches=0.02)\n",
    "            plt.close(fig)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax.plot(x, tr_loss, color=c_train, linestyle=\"-\",  linewidth=lw_train, label=\"Train\")\n",
    "        ax.plot(x, va_loss, color=c_val,   linestyle=\"--\", linewidth=lw_val,   label=\"Validation\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend(frameon=False, loc=\"best\")\n",
    "        finalize_axes(ax)\n",
    "        save(fig, out_dir / f\"{prefix}_loss\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax.plot(x, tr_acc, color=c_train, linestyle=\"-\",  linewidth=lw_train, label=\"Train\")\n",
    "        ax.plot(x, va_acc, color=c_val,   linestyle=\"--\", linewidth=lw_val,   label=\"Validation\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Accuracy (%)\" if acc_in_percent else \"Accuracy\")\n",
    "\n",
    "        if acc_in_percent:\n",
    "            if smart_acc_ylim and np.nanmax(va_acc) > 85:\n",
    "                lo = max(0.0, np.floor((min(np.nanmin(tr_acc), np.nanmin(va_acc)) - 2.0) / 5.0) * 5.0)\n",
    "                hi = min(100.0, np.ceil((max(np.nanmax(tr_acc), np.nanmax(va_acc)) + 1.0) / 5.0) * 5.0)\n",
    "                if hi - lo < 20:\n",
    "                    lo = max(0.0, hi - 20)\n",
    "                ax.set_ylim(lo, hi)\n",
    "            else:\n",
    "                ax.set_ylim(0.0, 100.0)\n",
    "        else:\n",
    "            ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "        if mark_best_val:\n",
    "            best_idx = int(np.nanargmax(va_acc))\n",
    "            ax.axvline(x[best_idx], linestyle=\":\", color=\"black\", linewidth=1.0, alpha=0.8)\n",
    "            ax.text(\n",
    "                x[best_idx], ax.get_ylim()[0],\n",
    "                f\"best@{int(x[best_idx])}\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=8\n",
    "            )\n",
    "\n",
    "        ax.legend(frameon=False, loc=\"best\")\n",
    "        finalize_axes(ax)\n",
    "        save(fig, out_dir / f\"{prefix}_accuracy\")\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    y_true: Union[Sequence[int], np.ndarray],\n",
    "    y_pred: Union[Sequence[int], np.ndarray],\n",
    "    out_path: Path,\n",
    "    *,\n",
    "    class_names: Optional[Sequence[str]] = None,\n",
    "    labels: Optional[Sequence[int]] = None,\n",
    "    normalize: bool = True,\n",
    "    show_counts: bool = True,\n",
    "    min_show_pct: float = 2.0,\n",
    "    min_show_count: int = 1,\n",
    "    dpi: int = 600,\n",
    "    cmap=\"Blues\",\n",
    "    tick_rotation: int = 45,\n",
    "    ):\n",
    "    rc = {\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times New Roman\", \"STIXGeneral\", \"DejaVu Serif\"],\n",
    "        \"mathtext.fontset\": \"stix\",\n",
    "        \"font.size\": 9,\n",
    "        \"axes.labelsize\": 10,\n",
    "        \"xtick.labelsize\": 8,\n",
    "        \"ytick.labelsize\": 8,\n",
    "        \"axes.linewidth\": 1.0,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"pdf.fonttype\": 42,\n",
    "        \"ps.fonttype\": 42,\n",
    "    }\n",
    "\n",
    "    with mpl.rc_context(rc):\n",
    "        out_path = Path(out_path)\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        y_true = np.asarray(y_true, dtype=int).reshape(-1)\n",
    "        y_pred = np.asarray(y_pred, dtype=int).reshape(-1)\n",
    "\n",
    "        if labels is None:\n",
    "            labels = np.unique(np.concatenate([y_true, y_pred])).tolist()\n",
    "        labels = list(labels)\n",
    "        n = len(labels)\n",
    "\n",
    "        label_to_idx = {lab: i for i, lab in enumerate(labels)}\n",
    "        y_true_idx = np.array([label_to_idx[y] for y in y_true])\n",
    "        y_pred_idx = np.array([label_to_idx[y] for y in y_pred])\n",
    "\n",
    "        if class_names is None:\n",
    "            if n > 26:\n",
    "                raise ValueError(\"Default a..z labels support up to 26 classes. Provide class_names explicitly.\")\n",
    "            class_names = list(string.ascii_lowercase[:n])\n",
    "        else:\n",
    "            if len(class_names) != n:\n",
    "                raise ValueError(\"len(class_names) must match number of classes\")\n",
    "\n",
    "        cm = confusion_matrix(y_true_idx, y_pred_idx, labels=range(n)).astype(float)\n",
    "\n",
    "        if normalize:\n",
    "            row_sums = cm.sum(axis=1, keepdims=True)\n",
    "            row_sums[row_sums == 0] = 1.0\n",
    "            cm_show = (cm / row_sums) * 100.0\n",
    "            vmin, vmax = 0.0, 100.0\n",
    "        else:\n",
    "            cm_show = cm\n",
    "            vmin, vmax = 0.0, float(np.nanmax(cm_show)) if np.isfinite(np.nanmax(cm_show)) else 1.0\n",
    "\n",
    "        fig_w = min(7.2, max(4.2, 0.42 * n + 1.9))\n",
    "        fig_h = min(7.2, max(3.8, 0.42 * n + 1.6))\n",
    "        fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "\n",
    "        im = ax.imshow(cm_show, interpolation=\"nearest\", cmap=cmap, aspect=\"equal\", vmin=vmin, vmax=vmax)\n",
    "        cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        cbar.ax.tick_params(labelsize=8)\n",
    "        if normalize:\n",
    "            cbar.set_label(\"%\", fontsize=9)\n",
    "\n",
    "        ax.set_xlabel(\"Predicted\")\n",
    "        ax.set_ylabel(\"True\")\n",
    "\n",
    "        ticks = np.arange(n)\n",
    "        ax.set_xticks(ticks)\n",
    "        ax.set_yticks(ticks)\n",
    "        ax.set_xticklabels(class_names, rotation=tick_rotation, ha=\"right\")\n",
    "        ax.set_yticklabels(class_names)\n",
    "\n",
    "        bbox_kw = dict(\n",
    "            boxstyle=\"round,pad=0.15\",\n",
    "            facecolor=\"white\",\n",
    "            edgecolor=\"none\",\n",
    "            alpha=0.75,\n",
    "        )\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                cnt = cm[i, j]\n",
    "                if normalize:\n",
    "                    pct = cm_show[i, j]\n",
    "                    if (pct < min_show_pct) and (cnt < min_show_count):\n",
    "                        continue\n",
    "                    text = f\"{pct:.1f}%\"\n",
    "                    if show_counts:\n",
    "                        text += f\"\\n({int(cnt)})\"\n",
    "                else:\n",
    "                    if cnt < min_show_count:\n",
    "                        continue\n",
    "                    text = f\"{int(cnt)}\"\n",
    "\n",
    "                ax.text(\n",
    "                    j, i, text,\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    fontsize=8 if n <= 10 else 7,\n",
    "                    color=\"black\",\n",
    "                    bbox=bbox_kw,\n",
    "                )\n",
    "\n",
    "        out_base = out_path.with_suffix(\"\")\n",
    "        fig.savefig(out_base.with_suffix(\".pdf\"), bbox_inches=\"tight\", pad_inches=0.02)\n",
    "        fig.savefig(out_base.with_suffix(\".png\"), dpi=dpi, bbox_inches=\"tight\", pad_inches=0.02)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Trainer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, cfg: TrainConfig):\n",
    "        self.cfg = cfg\n",
    "        seed_everything(cfg.seed, deterministic=cfg.deterministic)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        expected = make_run_name(cfg)\n",
    "        if cfg.out_dir.name != expected:\n",
    "            raise ValueError(f\"out_dir name mismatch:\\n expected={expected}\\n got={cfg.out_dir.name}\")\n",
    "\n",
    "        cfg.out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save config once during training\n",
    "        if cfg.mode == \"train\":\n",
    "            (cfg.out_dir / \"config.json\").write_text(\n",
    "                json.dumps(\n",
    "                    {k: str(v) if isinstance(v, Path) else v for k, v in asdict(cfg).items()},\n",
    "                    indent=2\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ---- Load processed splits (y is already int64 0..C-1) ----\n",
    "        (X_train, X_val, X_test,\n",
    "         y_train, y_val, y_test,\n",
    "         len_train, len_val, len_test) = load_raw_splits(cfg.data_dir)\n",
    "\n",
    "        # ---- Load label names from preprocessing output ----\n",
    "        label_map_path = cfg.data_dir / \"label_map.json\"\n",
    "        if not label_map_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Missing {label_map_path}. Your preprocessing should save label_map.json.\"\n",
    "            )\n",
    "\n",
    "        label_map = json.loads(label_map_path.read_text())\n",
    "        id2label = {int(k): v for k, v in label_map[\"id2label\"].items()}\n",
    "        num_classes = len(id2label)\n",
    "\n",
    "        # class_names in correct index order\n",
    "        self.class_names = [id2label[i] for i in range(num_classes)]\n",
    "\n",
    "        # Loaders\n",
    "        self.train_loader, self.val_loader, self.test_loader = make_loaders(\n",
    "            X_train, y_train, len_train,\n",
    "            X_val,   y_val,   len_val,\n",
    "            X_test,  y_test,  len_test,\n",
    "            cfg.batch_size,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        # Model\n",
    "        self.model = CNN_BiLSTM(\n",
    "            num_features=cfg.num_features,\n",
    "            num_classes=num_classes,\n",
    "            hidden_size=cfg.hidden_size,\n",
    "            dropout=cfg.dropout,\n",
    "            use_batchnorm=cfg.use_batchnorm,   \n",
    "        ).to(self.device)\n",
    "\n",
    "        # Sanity-check BN toggle\n",
    "        has_bn = any(isinstance(m, nn.BatchNorm1d) for m in self.model.modules())\n",
    "        if cfg.use_batchnorm != has_bn:\n",
    "            raise RuntimeError(\n",
    "                f\"BN mismatch: cfg.use_batchnorm={cfg.use_batchnorm}, model_has_bn={has_bn}\"\n",
    "            )\n",
    "        \n",
    "        print(f\"[Model] use_batchnorm={cfg.use_batchnorm} | detected_bn={has_bn}\")\n",
    "\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer,\n",
    "            step_size=cfg.step_size,\n",
    "            gamma=cfg.gamma\n",
    "        )\n",
    "\n",
    "        self.best_val_metric = -1.0   # macro F1\n",
    "        self.best_val_acc = -1.0\n",
    "        self.best_path = cfg.out_dir / \"best.pth\"\n",
    "        self.log_path  = cfg.out_dir / \"train_log.csv\"\n",
    "\n",
    "    def fit(self):\n",
    "        patience_left = self.cfg.patience\n",
    "        rows = []\n",
    "\n",
    "        for epoch in range(1, self.cfg.epochs + 1):\n",
    "            tr_loss, tr_acc, tr_acck = train_one_epoch(\n",
    "                self.model,\n",
    "                self.train_loader,\n",
    "                self.device,\n",
    "                self.criterion,\n",
    "                self.optimizer,\n",
    "                topk=self.cfg.topk\n",
    "            )\n",
    "\n",
    "            va_loss, va_acc, va_acck, va_macro_f1, va_weighted_f1, _, _ = evaluate(\n",
    "                self.model,\n",
    "                self.val_loader,\n",
    "                self.device,\n",
    "                self.criterion,\n",
    "                topk=self.cfg.topk\n",
    "            )\n",
    "\n",
    "            rows.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": tr_loss,\n",
    "                \"train_acc\": tr_acc,\n",
    "                \"train_acck\": tr_acck,\n",
    "                \"val_loss\": va_loss,\n",
    "                \"val_acc\": va_acc,\n",
    "                \"val_acck\": va_acck,\n",
    "                \"val_macro_f1\": va_macro_f1,\n",
    "                \"val_weighted_f1\": va_weighted_f1,\n",
    "                \"lr\": self.optimizer.param_groups[0][\"lr\"],\n",
    "            })\n",
    "            pd.DataFrame(rows).to_csv(self.log_path, index=False)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d}/{self.cfg.epochs} | \"\n",
    "                f\"tr_loss={tr_loss:.4f} tr_acc={tr_acc:.4f} | \"\n",
    "                f\"va_loss={va_loss:.4f} va_acc={va_acc:.4f} va_macroF1={va_macro_f1:.4f}\"\n",
    "            )\n",
    "\n",
    "            val_metric = float(va_macro_f1)  # early stop on macro-F1\n",
    "            if val_metric > self.best_val_metric:\n",
    "                self.best_val_metric = val_metric\n",
    "                self.best_val_acc = float(va_acc)\n",
    "                torch.save(self.model.state_dict(), self.best_path)\n",
    "                patience_left = self.cfg.patience\n",
    "            else:\n",
    "                patience_left -= 1\n",
    "                if patience_left <= 0:\n",
    "                    print(\n",
    "                        f\"Early stopping at epoch {epoch} \"\n",
    "                        f\"(best val_macroF1={self.best_val_metric:.4f}, best val_acc={self.best_val_acc:.4f})\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "        return self.log_path\n",
    "\n",
    "    def test(self):\n",
    "        if not self.best_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing {self.best_path}. Train first (or point to a run_dir with best.pth).\")\n",
    "\n",
    "        self.model.load_state_dict(torch.load(self.best_path, map_location=self.device))\n",
    "\n",
    "        te_loss, te_acc, te_acck, te_macro_f1, te_weighted_f1, y_true, y_pred = evaluate(\n",
    "            self.model,\n",
    "            self.test_loader,\n",
    "            self.device,\n",
    "            self.criterion,\n",
    "            topk=self.cfg.topk\n",
    "        )\n",
    "\n",
    "        report_txt = classification_report(\n",
    "            y_true, y_pred,\n",
    "            digits=4,\n",
    "            zero_division=0,\n",
    "            target_names=[str(c) for c in self.class_names]\n",
    "        )\n",
    "\n",
    "        # If log exists, refresh best vals from it (optional but fine)\n",
    "        if self.log_path.exists():\n",
    "            df = pd.read_csv(self.log_path)\n",
    "            if \"val_macro_f1\" in df.columns and len(df) > 0:\n",
    "                self.best_val_metric = float(df[\"val_macro_f1\"].max())\n",
    "            if \"val_acc\" in df.columns and len(df) > 0:\n",
    "                self.best_val_acc = float(df[\"val_acc\"].max())\n",
    "\n",
    "        (self.cfg.out_dir / \"test_summary.txt\").write_text(\n",
    "            f\"Best val_macroF1: {self.best_val_metric:.4f}\\n\"\n",
    "            f\"Best val_acc: {self.best_val_acc:.4f}\\n\"\n",
    "            f\"Test loss: {te_loss:.4f}\\n\"\n",
    "            f\"Test acc: {te_acc:.4f}\\n\"\n",
    "            f\"Test acc@{self.cfg.topk}: {te_acck:.4f}\\n\"\n",
    "            f\"Test macro F1: {te_macro_f1:.4f}\\n\"\n",
    "            f\"Test weighted F1: {te_weighted_f1:.4f}\\n\\n\"\n",
    "            f\"{report_txt}\\n\"\n",
    "        )\n",
    "\n",
    "        np.save(self.cfg.out_dir / \"y_true_test.npy\", y_true)\n",
    "        np.save(self.cfg.out_dir / \"y_pred_test.npy\", y_pred)\n",
    "\n",
    "        print(f\"TEST | loss={te_loss:.4f} acc={te_acc:.4f} macroF1={te_macro_f1:.4f}\")\n",
    "        return te_loss, te_acc, y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    cfg = TrainConfig(\n",
    "        data_dir=Path(\"data/processed_imu\"),\n",
    "        out_dir=Path(\"results\"),\n",
    "        batch_size=16,\n",
    "        use_batchnorm=False,\n",
    "        dropout=0.3,\n",
    "        seed=42,\n",
    "        topk=3,\n",
    "        epochs=30,\n",
    "        deterministic=True,\n",
    "        mode=\"train\",\n",
    "    )\n",
    "\n",
    "    cfg.out_dir = cfg.out_dir / make_run_name(cfg)\n",
    "    trainer = Trainer(cfg)\n",
    "    trainer.fit()\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] use_batchnorm=False | detected_bn=False\n",
      "Epoch   1/30 | tr_loss=2.6926 tr_acc=0.1729 | va_loss=1.9648 va_acc=0.3406 va_macroF1=0.2857\n",
      "Epoch   2/30 | tr_loss=1.5561 tr_acc=0.4816 | va_loss=1.0556 va_acc=0.6575 va_macroF1=0.6448\n",
      "Epoch   3/30 | tr_loss=0.8112 tr_acc=0.7459 | va_loss=0.5522 va_acc=0.8307 va_macroF1=0.8255\n",
      "Epoch   4/30 | tr_loss=0.5005 tr_acc=0.8410 | va_loss=0.3888 va_acc=0.8957 va_macroF1=0.8936\n",
      "Epoch   5/30 | tr_loss=0.3177 tr_acc=0.8977 | va_loss=0.3044 va_acc=0.9173 va_macroF1=0.9161\n",
      "Epoch   6/30 | tr_loss=0.2185 tr_acc=0.9345 | va_loss=0.1998 va_acc=0.9449 va_macroF1=0.9452\n",
      "Epoch   7/30 | tr_loss=0.1689 tr_acc=0.9480 | va_loss=0.1679 va_acc=0.9488 va_macroF1=0.9482\n",
      "Epoch   8/30 | tr_loss=0.1225 tr_acc=0.9636 | va_loss=0.1605 va_acc=0.9665 va_macroF1=0.9664\n",
      "Epoch   9/30 | tr_loss=0.0561 tr_acc=0.9818 | va_loss=0.0812 va_acc=0.9823 va_macroF1=0.9824\n",
      "Epoch  10/30 | tr_loss=0.0418 tr_acc=0.9882 | va_loss=0.0666 va_acc=0.9882 va_macroF1=0.9882\n",
      "Epoch  11/30 | tr_loss=0.0275 tr_acc=0.9911 | va_loss=0.1017 va_acc=0.9843 va_macroF1=0.9843\n",
      "Epoch  12/30 | tr_loss=0.0316 tr_acc=0.9937 | va_loss=0.0614 va_acc=0.9921 va_macroF1=0.9921\n",
      "Epoch  13/30 | tr_loss=0.0219 tr_acc=0.9953 | va_loss=0.0506 va_acc=0.9961 va_macroF1=0.9961\n",
      "Epoch  14/30 | tr_loss=0.0168 tr_acc=0.9958 | va_loss=0.1738 va_acc=0.9587 va_macroF1=0.9576\n",
      "Epoch  15/30 | tr_loss=0.0219 tr_acc=0.9945 | va_loss=0.1180 va_acc=0.9685 va_macroF1=0.9686\n",
      "Epoch  16/30 | tr_loss=0.0127 tr_acc=0.9966 | va_loss=0.1014 va_acc=0.9823 va_macroF1=0.9823\n",
      "Epoch  17/30 | tr_loss=0.0044 tr_acc=0.9987 | va_loss=0.0637 va_acc=0.9882 va_macroF1=0.9882\n",
      "Epoch  18/30 | tr_loss=0.0025 tr_acc=0.9996 | va_loss=0.0808 va_acc=0.9882 va_macroF1=0.9881\n",
      "Epoch  19/30 | tr_loss=0.0027 tr_acc=0.9996 | va_loss=0.0676 va_acc=0.9902 va_macroF1=0.9902\n",
      "Epoch  20/30 | tr_loss=0.0018 tr_acc=0.9996 | va_loss=0.0653 va_acc=0.9902 va_macroF1=0.9902\n",
      "Early stopping at epoch 20 (best val_macroF1=0.9961, best val_acc=0.9961)\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cfg_from_run_dir(run_dir: str | Path) -> TrainConfig:\n",
    "    run_dir = Path(run_dir)\n",
    "    cfg_path = run_dir / \"config.json\"\n",
    "    if not cfg_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {cfg_path}. This run_dir is not a valid experiment output folder.\")\n",
    "\n",
    "    d = json.loads(cfg_path.read_text())\n",
    "\n",
    "    d[\"data_dir\"] = Path(d[\"data_dir\"])\n",
    "    d[\"out_dir\"]  = Path(d[\"out_dir\"])\n",
    "\n",
    "    d[\"mode\"] = \"eval\"\n",
    "\n",
    "    return TrainConfig(**d)\n",
    "\n",
    "\n",
    "def evaluate_run(run_dir: str | Path):\n",
    "    cfg = load_cfg_from_run_dir(run_dir)\n",
    "    trainer = Trainer(cfg)\n",
    "    trainer.test()\n",
    "    print(f\"Evaluation completed for: {cfg.out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] use_batchnorm=True | detected_bn=True\n",
      "TEST | loss=0.1175 acc=0.9822 macroF1=0.9822\n",
      "Evaluation completed for: results\\exp_bn_bs16_seed42_drop03\n",
      "[Model] use_batchnorm=False | detected_bn=False\n",
      "TEST | loss=0.1488 acc=0.9763 macroF1=0.9765\n",
      "Evaluation completed for: results\\exp_no_bn_bs16_seed42_drop03\n"
     ]
    }
   ],
   "source": [
    "evaluate_run(\"results/exp_bn_bs16_seed42_drop03\")\n",
    "evaluate_run(\"results/exp_no_bn_bs16_seed42_drop03\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Draw Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(run_dir: Union[str, Path]):\n",
    "    OUT_DIR = Path(run_dir)\n",
    "\n",
    "    log_csv = OUT_DIR / \"train_log.csv\"\n",
    "    y_true = np.load(OUT_DIR / \"y_true_test.npy\")\n",
    "    y_pred = np.load(OUT_DIR / \"y_pred_test.npy\")\n",
    "\n",
    "    # Load class names from the SAME data_dir used for training\n",
    "    cfg_path = OUT_DIR / \"config.json\"\n",
    "    if not cfg_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {cfg_path}\")\n",
    "\n",
    "    cfg_json = json.loads(cfg_path.read_text())\n",
    "    data_dir = Path(cfg_json[\"data_dir\"])\n",
    "\n",
    "    label_map_path = data_dir / \"label_map.json\"\n",
    "    if not label_map_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {label_map_path}\")\n",
    "\n",
    "    label_map = json.loads(label_map_path.read_text())\n",
    "    id2label = {int(k): v for k, v in label_map[\"id2label\"].items()}\n",
    "    class_names = [id2label[i] for i in range(len(id2label))]\n",
    "\n",
    "    prefix = OUT_DIR.name\n",
    "\n",
    "    plot_training_curves_csv(log_csv, OUT_DIR, prefix)\n",
    "\n",
    "    labels = list(range(len(class_names)))\n",
    "\n",
    "    plot_confusion_matrix(\n",
    "        y_true, y_pred,\n",
    "        OUT_DIR / \"confusion_matrix_counts\",\n",
    "        class_names=class_names,\n",
    "        labels=labels,\n",
    "        normalize=False\n",
    "    )\n",
    "\n",
    "    plot_confusion_matrix(\n",
    "        y_true, y_pred,\n",
    "        OUT_DIR / \"confusion_matrix_normalized\",\n",
    "        class_names=class_names,\n",
    "        labels=labels,\n",
    "        normalize=True,\n",
    "        show_counts=True,\n",
    "        min_show_pct=10.0\n",
    "    )\n",
    "\n",
    "    print(f\"Plots generated for {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots generated for results\\exp_bn_bs16_seed42_drop03\n",
      "Plots generated for results\\exp_no_bn_bs16_seed42_drop03\n"
     ]
    }
   ],
   "source": [
    "plot_results(\"results/exp_bn_bs16_seed42_drop03\")\n",
    "plot_results(\"results/exp_no_bn_bs16_seed42_drop03\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
